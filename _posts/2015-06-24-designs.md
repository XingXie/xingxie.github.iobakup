---
layout: post
title:  "Framework Design Notes"
date:   2015-06-24
categories: Framework
excerpt: 
comments: true
---

### CAP
* Consistency (all nodes see the same data at the same time)
* Availability (a guarantee that every request receives a response about whether it succeeded or failed)
* Partition tolerance (the system continues to operate despite arbitrary partitioning due to network failures)

1. Stering and communication. Open-ended. Consider the following points. And be creative.

* Concurrency. Do you understand threads, deadlock, and starvation? Do you know how to parallelize algorithms? Do you understand consistency and coherence?
* Networking. Do you roughly understand IPC and TCP/IP? Do you know the difference between throughput and latency, and when each is the relevant factor?
* Abstraction. You should understand the systems you’re building upon. Do you know roughly how an OS, file system, and database work? Do you know about the various levels of caching in a modern OS?
* Real-World Performance. You should be familiar with the speed of everything your computer can do, including the relative performance of RAM, disk, SSD and your network.
* Estimation. Estimation, especially in the form of a back-of-the-envelope calculation, is important because it helps you narrow down the list of possible solutions to only the ones that are feasible. Then you have only a few prototypes or micro-benchmarks to write.
* Availability and Reliability. Are you thinking about how things can fail, especially in adistributed environment? Do know how to design a system to cope with network failures? Do you understand durability

### system design

0. Sterring and commucation.

1. constraints and use cases
2. Abstract design: draw diagrams and components and connections. And provide application layer, basic services, backend db design. 
3. understand bottleneck:  Perhaps your system needs a load balancer and many machines behind it to handle the user requests. Or maybe the data is so huge that you need to distribute your database on multiple machines. What are some of the downsides that occur from doing that? Is the database too slow and does it need some in-memory caching?
4. scalable abstract design: solve the bottle neck.
Be prepared for discussions about tradeoffs, about pros and cons. Be prepared to give alternatives, to ask questions, to identify and solve bottlenecks, to go broad or deep depending on your interviewer's preferences.
Don't get defensive: whenever your interviewer challenges your architectural choices, acknowledge that rarely an idea is perfect, and outline the advantages and disadvantages of your choice. Be open to new constraints to pop up during the discussion and to adjust your architecture on the fly.

### scalability basics
1. clone: Capistrano, AWS calls this AMI - Amazon Machine Image
2. db: Path #2 means to denormalize right from the beginning and include no more Joins in any database query. You can stay with MySQL, and use it like a NoSQL database, or you can switch to a better and easier to scale NoSQL database like MongoDB or CouchDB. Joins will now need to be done in your application code. The sooner you do this step the less code you will have to change in the future. But even if you successfully switch to the latest and greatest NoSQL database and let your app do the dataset-joins, soon your database requests will again be slower and slower. You will need to introduce a cache.
3. cache: With “cache” I always mean in-memory caches like Memcached or Redis. Please never do file-based caching, it makes cloning and auto-scaling of your servers just a pain. 
choice 1: cache db queries, hashed key is the query. Problem: updated data. 2. cache objects, better. 
And the best part: it makes asynchronous processing possible! Just imagine an army of worker servers who assemble your objects for you! The application just consumes the latest cached object and nearly never touches the databases anymore!
4. asynchronism case 1: cron job case 2: mq.If you now want to dive more into the details and actual technical design, RabbitMQ is one of many systems which help to implement async processing. You could also use ActiveMQ or a simple Redis list. The basic idea is to have a queue of tasks or jobs that a worker can process. Asynchronism seems complicated, but it is definitely worth your time to learn about it and implement it yourself. Backends become nearly infinitely scalable and frontends become snappy which is good for the overall user experience. 

scale horizontally: add nodes. vertically: add cpus or memory in one node.

### web distributed system design

key principles: availability, performance, reliability, scalability, manageability, cost. (rapsmc)

partition data problem: data locality, need to fetch data. Inconsistency

global cache: to overcome local cache missing by load balancer

proxy once the cache is not found: collaborate similar request One way to use a proxy to speed up data access is to collapse the same (or similar) requests together into one request, and then return the single result to the requesting clients. This is known as collapsed forwarding. but generally it is best to put the cache in front of the proxy, for the same reason that it is best to let the faster runners start first in a crowded marathon race. 

index:

load balancers: issue: user-session-specific data. managing user-session-specific data. In an e-commerce site, when you only have one client it is very easy to allow users to put things in their shopping cart and persist those contents between visits (which is important, because it is much more likely you will sell the product if it is still in the user's cart when they return). 

queue:

### scalability trade-offs

1. performance vs scalability
2. latency vs throughput: maximum throughput with acceptable latency
3. availability vs. consistency
availability pattern: fail-over, replication
scalability pattern: state (partitioning, http caching, rdbms sharding, nosql, distributed caching, data grids, concurrency)

### Concurrency: 

1. shared-state: indeterministic, introduce locks(problem) 
2. message-passing: actors: has msg queues, communicate with msgs, dataflow, 
software transactional memory: transaction can nest and compose

![alt text](https://cloud.githubusercontent.com/assets/5607138/8345396/3dd6f8b2-1aa3-11e5-88b9-22b55219fa89.png)

scalability patterns: behavior

ESB: enterprise service bus

stability patterns: timeouts, circuit breaker, let it crash, fail fast, bulkheads, steady state (logging), throttling (queue requests in diff steps)

### google online document 

other methods: 
1. event update: not reliable , not robust
2. 3 way merge, (C,S, version 1) -> version 2. problem: no realtime, on feedback from server, need to stop typing and cannot update live.
3. Solution: 

### two algo: diff and fuzzy patch.

shadow and backup for each client and server. version number n (client) and m(server), when send out an update edits, n++, m remain the same. And server receives the edits and compare the version m of server, if m is smaller in server, it will use backup to rollback server version. Patch the updates, diff the shadow with server, update the backup/shadow/server, m++, send edits back to client. And the client will do the similar thing.

Note: only client will send update to servers, servers are not positive.

For each client, server will have one to one shadow for them.

Limitation: attributions(cannot figure out who did what in server), (one package only in the route) might be big latency if c,s has long distance.

### redis

key --> data structure, provide list, set, etc and its operations. provide score(weight) to the the value.

has publish/subscribe pattern.

For example if there are more querying required, it mostly means more work in Redis, where you can use different data structures to suit your queries. Same is easier in MongoDB. On other hand this is often extra work in Redis would most likely to pay off with sheer speed.

MongoDB offers simplicity, much smaller learning for guys with SQL experience. Whereas Redis offers non-traditional approach hence more learning but huge flexibility.

Eg. A cache layer can probably be better implemented in Redis, and as for a more schema-able data MongoDB might be better. [Note: mongodb is schemaless] 
 Programming with Redis is just like doing everything with Lists and Hashes inside memory with your favorite dynamic programming language, but the dataset is persistent and of course not as fast as accessing directly to your PC's memory (there is a networking layer in the middle). 

### memcached

* storage cost
* retrieval cost
* invalidation
* replacement policy
* cold/warm cache

typical used in session, user and homepage data.

### twitter trending topics

* bad method: parametric: detect a jump with a theadhold . Problem: gradual rise and no clear jump, diff model.
* good method: data-driven: compute the distance between vote and no-vote to fit one module.

### [mapreduce vs spark](http://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/)

### Bloom filter

A Bloom filter is a space-efficient probabilistic data structure, conceived by Burton Howard Bloom in 1970, that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not, thus a Bloom filter has a 100% recall rate. In other words, a query returns either "possibly in set" or "definitely not in set". Elements can be added to the set, but not removed (though this can be addressed with a "counting" filter). The more elements that are added to the set, the larger the probability of false positives.

Bloom filters have a strong space advantage over other data structures for representing sets, such as self-balancing binary search trees, tries, hash tables, or simple arrays or linked lists of the entries. Most of these require storing at least the data items themselves, which can require anywhere from a small number of bits, for small integers, to an arbitrary number of bits, such as for strings (tries are an exception, since they can share storage between elements with equal prefixes). Linked structures incur an additional linear space overhead for pointers. A Bloom filter with 1% error and an optimal value of k, on the other hand, requires only about 9.6 bits per element — regardless of the size of the elements. This advantage comes partly from its compactness, inherited from arrays, and partly from its probabilistic nature. If a 1% false positive rate seems too high, each time we add about 4.8 bits per element we decrease it by ten times.
